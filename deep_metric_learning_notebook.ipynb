{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff55f3eb",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9a06f",
   "metadata": {},
   "source": [
    "Classical machine learning classification uses cross entropy loss. It works well when classes are well-seperated, but struggles in real-world scenarios where:\n",
    "1. High intra-class variance: Samples from the same class look very different(e.g; the same person photographed from different angles,lightening and angles).\n",
    "2. Low inter-class variance: Samples from different classes look similar(e.g., two different people who happen to look alike).\n",
    "\n",
    "Applications like face recognition, fingerprint matching, image retrieval, and  person re-identification all face this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8b9e3",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss: Complete Derivation\n",
    "\n",
    "## Part 1: The Classification Setup\n",
    "\n",
    "Consider a neural network for classification with $C$ classes.\n",
    "\n",
    "Given an input $x$, the network produces a **logit** (raw score) for each class:\n",
    "\n",
    "$$z_j = W_j^T f + b_j \\quad \\text{for class } j \\in \\{1, 2, \\ldots, C\\}$$\n",
    "\n",
    "where:\n",
    "- $f$ is the feature vector (output of the network before the final layer)\n",
    "- $W_j$ is the weight vector for class $j$\n",
    "- $b_j$ is the bias for class $j$\n",
    "\n",
    "## Part 2: The Problem — Converting Scores to Probabilities\n",
    "\n",
    "We need to convert logits $z_1, z_2, \\ldots, z_C$ into probabilities.\n",
    "\n",
    "**Requirements for valid probabilities:**\n",
    "1. Each probability must be non-negative: $P(y=j|x) \\geq 0$\n",
    "2. Probabilities must sum to 1: $\\sum_{j=1}^{C} P(y=j|x) = 1$\n",
    "\n",
    "**Why can't we just normalize the logits directly?**\n",
    "\n",
    "$$P(y=j|x) = \\frac{z_j}{\\sum_k z_k} \\quad \\text{❌ WRONG}$$\n",
    "\n",
    "**Problem**: If $z_j = -5$, we get a negative probability, which is invalid.\n",
    "\n",
    "## Part 3: The Softmax Function\n",
    "\n",
    "### Step 1: Make Everything Positive\n",
    "\n",
    "Apply the exponential function to each logit:\n",
    "\n",
    "$$e^{z_j} > 0 \\quad \\text{for any } z_j \\in \\mathbb{R}$$\n",
    "\n",
    "This maps any real number to a positive number.\n",
    "\n",
    "### Step 2: Normalize\n",
    "\n",
    "Divide by the sum to ensure probabilities sum to 1:\n",
    "\n",
    "$$\\boxed{P(y=j|x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}}$$\n",
    "\n",
    "This is the **softmax function**.\n",
    "\n",
    "### Why Exponential Specifically?\n",
    "\n",
    "The exponential function has a special property. If we compute the log-odds between two classes:\n",
    "\n",
    "$$\\log \\frac{P(y=j|x)}{P(y=k|x)} = \\log \\frac{e^{z_j}}{e^{z_k}} = z_j - z_k$$\n",
    "\n",
    "The **difference in logits equals the log-odds**. This gives a clean, interpretable relationship.\n",
    "\n",
    "### Properties of Softmax\n",
    "\n",
    "| Property | Explanation |\n",
    "|----------|-------------|\n",
    "| Outputs in $(0, 1)$ | Each output is a valid probability |\n",
    "| Sum to 1 | $\\sum_j P(y=j\\|x) = 1$ |\n",
    "| Preserves ranking | Higher logit → higher probability |\n",
    "| Differentiable | Enables gradient-based optimization |\n",
    "\n",
    "\n",
    "## Part 4: Maximum Likelihood Estimation\n",
    "\n",
    "### The Goal\n",
    "\n",
    "We have training data: $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\}$\n",
    "\n",
    "We want to find parameters $\\theta = \\{W_j, b_j\\}$ that best explain this data.\n",
    "\n",
    "### The Likelihood Function\n",
    "\n",
    "**Key question**: Given parameters $\\theta$, what is the probability that our model would produce the observed labels?\n",
    "\n",
    "For a single sample $(x_i, y_i)$:\n",
    "\n",
    "$$L_i(\\theta) = P(y_i | x_i; \\theta)$$\n",
    "\n",
    "This is the probability our model assigns to the **true class**.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose for sample $i$:\n",
    "- True class: \"cat\"\n",
    "- Model outputs: $P(\\text{cat}) = 0.8$, $P(\\text{dog}) = 0.15$, $P(\\text{bird}) = 0.05$\n",
    "\n",
    "Then $L_i(\\theta) = 0.8$\n",
    "\n",
    "### Likelihood for the Entire Dataset\n",
    "\n",
    "Assuming samples are **independent**, the joint probability is the product:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{N} P(y_i | x_i; \\theta)$$\n",
    "\n",
    "**Why multiply?** For independent events A and B:\n",
    "\n",
    "$$P(A \\text{ and } B) = P(A) \\times P(B)$$\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Principle**: Choose parameters that maximize the likelihood.\n",
    "\n",
    "$$\\hat{\\theta} = \\arg\\max_{\\theta} \\prod_{i=1}^{N} P(y_i | x_i; \\theta)$$\n",
    "\n",
    "\n",
    "## Part 5: Log-Likelihood\n",
    "\n",
    "### The Numerical Problem\n",
    "\n",
    "With $N = 10000$ samples:\n",
    "\n",
    "$$L(\\theta) = 0.8 \\times 0.7 \\times 0.9 \\times \\ldots \\approx 10^{-500}$$\n",
    "\n",
    "This causes **numerical underflow** — computers can't represent such small numbers.\n",
    "\n",
    "### The Solution: Take the Logarithm\n",
    "\n",
    "Since $\\log$ is a **monotonically increasing function**:\n",
    "\n",
    "$$\\arg\\max_\\theta L(\\theta) = \\arg\\max_\\theta \\log L(\\theta)$$\n",
    "\n",
    "The parameters that maximize $L$ also maximize $\\log L$.\n",
    "\n",
    "### Log of a Product Becomes a Sum\n",
    "\n",
    "$$\\log L(\\theta) = \\log \\prod_{i=1}^{N} P(y_i|x_i) = \\sum_{i=1}^{N} \\log P(y_i|x_i)$$\n",
    "\n",
    "Now we're adding log-probabilities instead of multiplying probabilities. Much more stable!\n",
    "\n",
    "\n",
    "## Part 6: From Log-Likelihood to Cross-Entropy Loss\n",
    "\n",
    "### Maximizing vs. Minimizing\n",
    "\n",
    "In machine learning, we typically **minimize a loss function** rather than maximize likelihood.\n",
    "\n",
    "Simple fix — take the negative:\n",
    "\n",
    "$$\\text{Negative Log-Likelihood} = -\\log L(\\theta) = -\\sum_{i=1}^{N} \\log P(y_i|x_i)$$\n",
    "\n",
    "Minimizing this is equivalent to maximizing the likelihood.\n",
    "\n",
    "### Cross-Entropy Loss (Single Sample)\n",
    "\n",
    "For one sample with true class $y$:\n",
    "\n",
    "$$L_{CE} = -\\log P(y|x)$$\n",
    "\n",
    "Substituting the softmax:\n",
    "\n",
    "$$L_{CE} = -\\log \\frac{e^{z_y}}{\\sum_{k=1}^{C} e^{z_k}}$$\n",
    "\n",
    "### Expanding the Logarithm\n",
    "\n",
    "Using $\\log \\frac{a}{b} = \\log a - \\log b$:\n",
    "\n",
    "$$L_{CE} = -\\left( \\log e^{z_y} - \\log \\sum_{k=1}^{C} e^{z_k} \\right)$$\n",
    "\n",
    "$$\\boxed{L_{CE} = -z_y + \\log \\sum_{k=1}^{C} e^{z_k}}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- $-z_y$: We want the logit of the true class to be **high** (so this term is small)\n",
    "- $\\log \\sum_k e^{z_k}$: We want other logits to be **low** (so this term is small)\n",
    "\n",
    "\n",
    "## Part 7: General Form with One-Hot Encoding\n",
    "\n",
    "### One-Hot Representation\n",
    "\n",
    "Instead of representing the true class as a single integer $y$, we use a **one-hot vector**:\n",
    "\n",
    "$$\\mathbf{y} = [y_1, y_2, \\ldots, y_C]$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$y_j = \\begin{cases} 1 & \\text{if } j \\text{ is the true class} \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "For 3 classes (cat, dog, bird) where the true class is \"dog\":\n",
    "\n",
    "$$\\mathbf{y} = [0, 1, 0]$$\n",
    "\n",
    "### Cross-Entropy with One-Hot Encoding\n",
    "\n",
    "$$L_{CE} = -\\sum_{j=1}^{C} y_j \\log P(y=j|x)$$\n",
    "\n",
    "Since $y_j = 0$ for all classes except the true class, this simplifies to:\n",
    "\n",
    "$$L_{CE} = -\\log P(y=\\text{true class}|x)$$\n",
    "\n",
    "which matches our earlier formula.\n",
    "\n",
    "### Full Formula\n",
    "\n",
    "$$\\boxed{L_{CE} = -\\sum_{j=1}^{C} y_j \\log \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}}$$\n",
    "\n",
    "\n",
    "## Part 8: Connection to Information Theory\n",
    "\n",
    "### What is Entropy?\n",
    "\n",
    "For a probability distribution $p$, **entropy** measures uncertainty:\n",
    "\n",
    "$$H(p) = -\\sum_j p_j \\log p_j$$\n",
    "\n",
    "### What is Cross-Entropy?\n",
    "\n",
    "**Cross-entropy** between true distribution $p$ and predicted distribution $q$:\n",
    "\n",
    "$$H(p, q) = -\\sum_j p_j \\log q_j$$\n",
    "\n",
    "In classification:\n",
    "- $p$ = one-hot vector (true distribution, all mass on one class)\n",
    "- $q$ = softmax output (predicted distribution)\n",
    "\n",
    "So our loss is literally the cross-entropy between true and predicted distributions!\n",
    "\n",
    "### Relationship to KL Divergence\n",
    "\n",
    "$$H(p, q) = H(p) + D_{KL}(p \\| q)$$\n",
    "\n",
    "Since $H(p) = 0$ for one-hot vectors:\n",
    "\n",
    "$$L_{CE} = D_{KL}(p \\| q)$$\n",
    "\n",
    "Minimizing cross-entropy = minimizing KL divergence from true to predicted distribution.\n",
    "\n",
    "\n",
    "## Gradient of Cross-Entropy Loss\n",
    "\n",
    "For backpropagation, we need $\\frac{\\partial L_{CE}}{\\partial z_j}$.\n",
    "\n",
    "### Result\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial z_j} = P(y=j|x) - y_j = \\text{softmax}(z_j) - y_j$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "| True class? | $y_j$ | $P(y=j\\|x)$ | Gradient |\n",
    "|-------------|-------|-------------|----------|\n",
    "| Yes | 1 | e.g., 0.8 | $0.8 - 1 = -0.2$ (push up) |\n",
    "| No | 0 | e.g., 0.15 | $0.15 - 0 = 0.15$ (push down) |\n",
    "\n",
    "The gradient:\n",
    "- **Increases** the logit of the true class\n",
    "- **Decreases** the logits of incorrect classes\n",
    "- Magnitude depends on how wrong the prediction is\n",
    "\n",
    "\n",
    "##  Summary\n",
    "\n",
    "| Step | What We Do | Why |\n",
    "|------|------------|-----|\n",
    "| 1. Compute logits | $z_j = W_j^T f + b_j$ | Linear transformation of features |\n",
    "| 2. Apply softmax | $P(j\\|x) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ | Convert to valid probabilities |\n",
    "| 3. Compute loss | $L = -\\log P(\\text{true class}\\|x)$ | Negative log-likelihood |\n",
    "| 4. Backpropagate | $\\nabla_z L = \\text{softmax}(z) - y$ | Update parameters |\n",
    "\n",
    "### The Complete Pipeline\n",
    "\n",
    "```\n",
    "Input x\n",
    "    ↓\n",
    "Neural Network → Feature f\n",
    "    ↓\n",
    "Linear Layer → Logits z = Wf + b\n",
    "    ↓\n",
    "Softmax → Probabilities P(y|x)\n",
    "    ↓\n",
    "Cross-Entropy Loss with true label y\n",
    "    ↓\n",
    "Backpropagation to update W, b\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Softmax** converts unconstrained logits to valid probabilities\n",
    "2. **Cross-entropy loss** comes from maximum likelihood estimation\n",
    "3. **Log transformation** converts products to sums for numerical stability\n",
    "4. **Negative sign** converts maximization to minimization\n",
    "5. The gradient has an elegant form: $\\text{prediction} - \\text{target}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532c989",
   "metadata": {},
   "source": [
    "# Softmax Classification vs. Metric Learning: Complete Comparison\n",
    "\n",
    "## 1. Overview Comparison\n",
    "\n",
    "| Aspect | Softmax Classification | Metric Learning |\n",
    "|--------|------------------------|-----------------|\n",
    "| **Goal** | Assign correct class label | Learn discriminative embedding space |\n",
    "| **Training unit** | Single sample $(x, y)$ | Sample pairs or triplets $(x_a, x_p, x_n)$ |\n",
    "| **Output** | Class probabilities | Feature embeddings |\n",
    "| **Prediction** | Highest probability class | Nearest neighbor in embedding space |\n",
    "| **New classes** | ❌ Cannot handle | ✅ Can handle without retraining |\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mathematical Formulation\n",
    "\n",
    "### 2.1 Forward Pass\n",
    "\n",
    "| Step | Softmax Classification | Metric Learning (Triplet) |\n",
    "|------|------------------------|---------------------------|\n",
    "| **Input** | Single image $x$ | Three images: anchor $x_a$, positive $x_p$, negative $x_n$ |\n",
    "| **Feature extraction** | $f = \\text{CNN}(x)$ | $f_a = \\text{CNN}(x_a)$, $f_p = \\text{CNN}(x_p)$, $f_n = \\text{CNN}(x_n)$ |\n",
    "| **Additional layer** | Linear: $z_j = W_j^T f + b_j$ | Normalize: $\\hat{f} = \\frac{f}{\\|\\|f\\|\\|}$ (optional) |\n",
    "| **Final transformation** | Softmax: $P(y=j\\|x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}$ | Distance: $d_{ap} = \\|\\|f_a - f_p\\|\\|^2$, $d_{an} = \\|\\|f_a - f_n\\|\\|^2$ |\n",
    "\n",
    "### 2.2 Loss Functions\n",
    "\n",
    "| Aspect | Softmax (Cross-Entropy) | Metric Learning (Triplet) |\n",
    "|--------|-------------------------|---------------------------|\n",
    "| **Formula** | $L_{CE} = -\\log P(y\\|x) = -\\log \\frac{e^{z_y}}{\\sum_{k=1}^{C} e^{z_k}}$ | $L_{triplet} = \\max(0, \\|\\|f_a - f_p\\|\\|^2 - \\|\\|f_a - f_n\\|\\|^2 + \\alpha)$ |\n",
    "| **Expanded form** | $L_{CE} = -z_y + \\log \\sum_{k=1}^{C} e^{z_k}$ | $L_{triplet} = [d_{ap} - d_{an} + \\alpha]_+$ |\n",
    "| **Samples involved** | ONE sample | THREE samples |\n",
    "| **Hyperparameters** | None | Margin $\\alpha$ |\n",
    "| **When loss = 0** | $P(y\\|x) = 1$ (perfect classification) | $d_{an} > d_{ap} + \\alpha$ (margin satisfied) |\n",
    "\n",
    "### 2.3 Gradient Formulas\n",
    "\n",
    "| Gradient | Softmax (Cross-Entropy) | Metric Learning (Triplet) |\n",
    "|----------|-------------------------|---------------------------|\n",
    "| **w.r.t. logits/distances** | $\\frac{\\partial L_{CE}}{\\partial z_j} = \\hat{p}_j - y_j$ | N/A (no logits) |\n",
    "| **w.r.t. features** | $\\frac{\\partial L_{CE}}{\\partial f} = \\sum_{j=1}^{C} \\hat{p}_j W_j - W_y$ | $\\frac{\\partial L}{\\partial f_a} = 2(f_n - f_p)$ |\n",
    "| **w.r.t. anchor** | N/A | $\\frac{\\partial L}{\\partial f_a} = 2(f_n - f_p)$ |\n",
    "| **w.r.t. positive** | N/A | $\\frac{\\partial L}{\\partial f_p} = 2(f_p - f_a)$ |\n",
    "| **w.r.t. negative** | N/A | $\\frac{\\partial L}{\\partial f_n} = 2(f_a - f_n)$ |\n",
    "| **w.r.t. weights** | $\\frac{\\partial L_{CE}}{\\partial W_j} = (\\hat{p}_j - y_j) \\cdot f$ | N/A (no class weights) |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. What Gets Optimized\n",
    "\n",
    "| Aspect | Softmax Classification | Metric Learning |\n",
    "|--------|------------------------|-----------------|\n",
    "| **Learnable parameters** | CNN weights + Class weights $W_j$ + Biases $b_j$ | CNN weights only |\n",
    "| **What gradients push** | Features toward class weights $W_j$ | Features toward same-class features |\n",
    "| **Optimization target** | $\\max P(\\text{correct class}\\|x)$ | $\\min d(\\text{same class})$, $\\max d(\\text{different class})$ |\n",
    "| **Sample relationships** | Implicit (through shared $W_j$) | Explicit (direct distance computation) |\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "SOFTMAX:                                    METRIC LEARNING:\n",
    "\n",
    "        W_cat ★                                   ●●● (cats clustered)\n",
    "             ↑                                    \n",
    "        f_cat ●                                   \n",
    "             ↑                                    \n",
    "        f_cat ●                                       ▲▲▲ (dogs clustered)\n",
    "             \n",
    "Features → Class Weights                    Features → Other Features\n",
    "(indirect relationship)                     (direct relationship)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Training Process\n",
    "\n",
    "| Step | Softmax Classification | Metric Learning |\n",
    "|------|------------------------|-----------------|\n",
    "| **Step 1** | Input: $(x, y)$ one image with label | Input: $(x_a, x_p, x_n)$ triplet of images |\n",
    "| **Step 2** | Extract feature: $f = \\text{CNN}(x)$ | Extract features: $f_a, f_p, f_n = \\text{CNN}(x_a, x_p, x_n)$ |\n",
    "| **Step 3** | Compute logits: $z_j = W_j^T f + b_j$ | Compute distances: $d_{ap}, d_{an}$ |\n",
    "| **Step 4** | Apply softmax: $P(y=j\\|x)$ | Check margin: $d_{ap} - d_{an} + \\alpha$ |\n",
    "| **Step 5** | Compute loss: $L = -\\log P(y\\|x)$ | Compute loss: $L = \\max(0, d_{ap} - d_{an} + \\alpha)$ |\n",
    "| **Step 6** | Backprop: Update CNN + $W_j$ | Backprop: Update CNN only |\n",
    "| **Effect** | $W_y$ moves toward $f$ | $f_p$ moves toward $f_a$, $f_n$ moves away |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Inference/Prediction\n",
    "\n",
    "| Aspect | Softmax Classification | Metric Learning |\n",
    "|--------|------------------------|-----------------|\n",
    "| **Method** | Compute class probabilities | Compute distances to stored embeddings |\n",
    "| **Formula** | $\\hat{y} = \\arg\\max_j P(y=j\\|x)$ | $\\hat{y} = \\arg\\min_j d(f_{query}, f_j)$ |\n",
    "| **Requirements** | Class weights $W_j$ must exist | Reference embeddings must be stored |\n",
    "| **Computation** | Forward pass + softmax | Forward pass + distance computation |\n",
    "\n",
    "### Inference Pipeline\n",
    "\n",
    "```\n",
    "SOFTMAX:\n",
    "┌───────┐    ┌─────┐    ┌────────┐    ┌─────────┐    ┌──────────┐\n",
    "│ Image │ →  │ CNN │ →  │ Linear │ →  │ Softmax │ →  │ argmax   │\n",
    "│   x   │    │     │    │  W·f+b │    │         │    │ = class  │\n",
    "└───────┘    └─────┘    └────────┘    └─────────┘    └──────────┘\n",
    "\n",
    "METRIC LEARNING:\n",
    "┌───────┐    ┌─────┐    ┌───────────┐    ┌─────────────────┐    ┌──────────┐\n",
    "│ Image │ →  │ CNN │ →  │ Normalize │ →  │ Distance to all │ →  │ nearest  │\n",
    "│   x   │    │     │    │   f/||f|| │    │ stored embed.   │    │ neighbor │\n",
    "└───────┘    └─────┘    └───────────┘    └─────────────────┘    └──────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Handling New Classes\n",
    "\n",
    "| Scenario | Softmax Classification | Metric Learning |\n",
    "|----------|------------------------|-----------------|\n",
    "| **New class appears** | ❌ Cannot predict (no $W_{new}$) | ✅ Can predict |\n",
    "| **Solution** | Retrain with new class | Add new embeddings to database |\n",
    "| **Effort required** | Full retraining | Just compute embeddings |\n",
    "| **Use case** | Fixed class problems | Open-set recognition |\n",
    "\n",
    "### Example: Face Recognition\n",
    "\n",
    "```\n",
    "SOFTMAX (Problematic):\n",
    "- Training: 1000 people → W_1, W_2, ..., W_1000\n",
    "- New employee joins → Need to retrain entire model!\n",
    "- Not scalable for millions of faces\n",
    "\n",
    "METRIC LEARNING (Solution):\n",
    "- Training: Learn good embedding function\n",
    "- New employee joins → Just store their face embedding\n",
    "- Scalable to billions of faces\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Loss Function Variants\n",
    "\n",
    "### 7.1 Classification Losses\n",
    "\n",
    "| Loss | Formula | Use Case |\n",
    "|------|---------|----------|\n",
    "| **Cross-Entropy** | $L = -\\log \\frac{e^{z_y}}{\\sum_k e^{z_k}}$ | Standard classification |\n",
    "| **Binary Cross-Entropy** | $L = -[y\\log\\hat{y} + (1-y)\\log(1-\\hat{y})]$ | Binary classification |\n",
    "| **Focal Loss** | $L = -\\alpha(1-\\hat{p})^\\gamma \\log(\\hat{p})$ | Class imbalance |\n",
    "\n",
    "### 7.2 Metric Learning Losses\n",
    "\n",
    "| Loss | Formula | Key Feature |\n",
    "|------|---------|-------------|\n",
    "| **Contrastive** | $L = \\begin{cases} d^2 & \\text{same class} \\\\ \\max(0, \\alpha - d)^2 & \\text{diff class} \\end{cases}$ | Pairs only |\n",
    "| **Triplet** | $L = \\max(0, d_{ap} - d_{an} + \\alpha)$ | Anchor-Positive-Negative |\n",
    "| **N-Pair** | $L = \\log(1 + \\sum_k e^{f_a \\cdot f_n^k - f_a \\cdot f_p})$ | Multiple negatives |\n",
    "| **Multi-Similarity** | $L = \\frac{1}{\\alpha}\\log[1 + \\sum e^{-\\alpha(S_{ip}-\\lambda)}] + \\frac{1}{\\beta}\\log[1 + \\sum e^{\\beta(S_{in}-\\lambda)}]$ | Weighted similarities |\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Gradient Behavior Comparison\n",
    "\n",
    "### 8.1 Effect on Correct/Incorrect Classes\n",
    "\n",
    "| Scenario | Softmax Gradient | Effect |\n",
    "|----------|------------------|--------|\n",
    "| Correct class, low confidence | $\\hat{p}_y - 1 = 0.2 - 1 = -0.8$ | Strong push UP |\n",
    "| Correct class, high confidence | $\\hat{p}_y - 1 = 0.95 - 1 = -0.05$ | Weak push up |\n",
    "| Wrong class, high score | $\\hat{p}_j - 0 = 0.7$ | Strong push DOWN |\n",
    "| Wrong class, low score | $\\hat{p}_j - 0 = 0.05$ | Weak push down |\n",
    "\n",
    "### 8.2 Effect on Triplet Samples\n",
    "\n",
    "| Sample | Gradient | Effect |\n",
    "|--------|----------|--------|\n",
    "| Anchor $f_a$ | $2(f_n - f_p)$ | Move toward positive, away from negative |\n",
    "| Positive $f_p$ | $2(f_p - f_a)$ | Pull toward anchor |\n",
    "| Negative $f_n$ | $2(f_a - f_n)$ | Push away from anchor |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Embedding Space Structure\n",
    "\n",
    "| Property | Softmax | Metric Learning |\n",
    "|----------|---------|-----------------|\n",
    "| **Intra-class distance** | Not explicitly minimized | Explicitly minimized |\n",
    "| **Inter-class distance** | Not explicitly maximized | Explicitly maximized |\n",
    "| **Cluster structure** | Side effect, not guaranteed | Main objective |\n",
    "| **Decision boundary** | Linear (in logit space) | Distance-based |\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "SOFTMAX (No guarantee on structure):     METRIC LEARNING (Structured):\n",
    "\n",
    "    ● cat                                     ●●● cats\n",
    "        ●cat                                  \n",
    "                    ▲dog                          ▲▲▲ dogs\n",
    "    ●cat                ▲dog                 \n",
    "                                                  ■■■ birds\n",
    "        ■bird   ■bird                        \n",
    "                                             \n",
    "Samples may be scattered                   Samples are clustered by class\n",
    "(classification still works)               (distances are meaningful)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. When to Use Which\n",
    "\n",
    "| Scenario | Recommended Approach | Reason |\n",
    "|----------|---------------------|--------|\n",
    "| Fixed classes, many samples per class | Softmax | Simple, effective |\n",
    "| Few samples per class | Metric Learning | Better generalization |\n",
    "| New classes at test time | Metric Learning | Can handle without retraining |\n",
    "| Face recognition | Metric Learning | Millions of identities |\n",
    "| Image retrieval | Metric Learning | Need meaningful distances |\n",
    "| Person re-identification | Metric Learning | Open-set problem |\n",
    "| Standard image classification | Softmax | Well-established |\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Summary Formula Sheet\n",
    "\n",
    "### Softmax Classification\n",
    "\n",
    "$$\\text{Logit: } z_j = W_j^T f + b_j$$\n",
    "\n",
    "$$\\text{Probability: } P(y=j|x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}$$\n",
    "\n",
    "$$\\text{Loss: } L_{CE} = -\\log P(y|x) = -z_y + \\log \\sum_{k=1}^{C} e^{z_k}$$\n",
    "\n",
    "$$\\text{Gradient (logits): } \\frac{\\partial L}{\\partial z_j} = \\hat{p}_j - y_j$$\n",
    "\n",
    "$$\\text{Gradient (features): } \\frac{\\partial L}{\\partial f} = \\sum_j \\hat{p}_j W_j - W_y$$\n",
    "\n",
    "$$\\text{Gradient (weights): } \\frac{\\partial L}{\\partial W_j} = (\\hat{p}_j - y_j) \\cdot f$$\n",
    "\n",
    "### Metric Learning (Triplet)\n",
    "\n",
    "$$\\text{Distance (positive): } d_{ap} = \\|f_a - f_p\\|^2$$\n",
    "\n",
    "$$\\text{Distance (negative): } d_{an} = \\|f_a - f_n\\|^2$$\n",
    "\n",
    "$$\\text{Loss: } L_{triplet} = \\max(0, d_{ap} - d_{an} + \\alpha)$$\n",
    "\n",
    "$$\\text{Gradient (anchor): } \\frac{\\partial L}{\\partial f_a} = 2(f_n - f_p)$$\n",
    "\n",
    "$$\\text{Gradient (positive): } \\frac{\\partial L}{\\partial f_p} = 2(f_p - f_a)$$\n",
    "\n",
    "$$\\text{Gradient (negative): } \\frac{\\partial L}{\\partial f_n} = 2(f_a - f_n)$$\n",
    "\n",
    "---\n",
    "\n",
    "## 12. Key Takeaway\n",
    "\n",
    "| | Softmax | Metric Learning |\n",
    "|-|---------|-----------------|\n",
    "| **One-line summary** | \"Which class template is this closest to?\" | \"Which other samples is this closest to?\" |\n",
    "| **Optimizes** | $P(\\text{correct class}\\|x)$ | $d(\\text{same class}) < d(\\text{different class})$ |\n",
    "| **Fundamental unit** | Single sample | Sample relationships |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47160d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec3671c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "791fb6f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
