{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff55f3eb",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9a06f",
   "metadata": {},
   "source": [
    "Classical machine learning classification uses cross entropy loss. It works well when classes are well-seperated, but struggles in real-world scenarios where:\n",
    "1. High intra-class variance: Samples from the same class look very different(e.g; the same person photographed from different angles,lightening and angles).\n",
    "2. Low inter-class variance: Samples from different classes look similar(e.g., two different people who happen to look alike).\n",
    "\n",
    "Applications like face recognition, fingerprint matching, image retrieval, and  person re-identification all face this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8b9e3",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss: Complete Derivation\n",
    "\n",
    "## Part 1: The Classification Setup\n",
    "\n",
    "Consider a neural network for classification with $C$ classes.\n",
    "\n",
    "Given an input $x$, the network produces a **logit** (raw score) for each class:\n",
    "\n",
    "$$z_j = W_j^T f + b_j \\quad \\text{for class } j \\in \\{1, 2, \\ldots, C\\}$$\n",
    "\n",
    "where:\n",
    "- $f$ is the feature vector (output of the network before the final layer)\n",
    "- $W_j$ is the weight vector for class $j$\n",
    "- $b_j$ is the bias for class $j$\n",
    "\n",
    "## Part 2: The Problem — Converting Scores to Probabilities\n",
    "\n",
    "We need to convert logits $z_1, z_2, \\ldots, z_C$ into probabilities.\n",
    "\n",
    "**Requirements for valid probabilities:**\n",
    "1. Each probability must be non-negative: $P(y=j|x) \\geq 0$\n",
    "2. Probabilities must sum to 1: $\\sum_{j=1}^{C} P(y=j|x) = 1$\n",
    "\n",
    "**Why can't we just normalize the logits directly?**\n",
    "\n",
    "$$P(y=j|x) = \\frac{z_j}{\\sum_k z_k} \\quad \\text{❌ WRONG}$$\n",
    "\n",
    "**Problem**: If $z_j = -5$, we get a negative probability, which is invalid.\n",
    "\n",
    "## Part 3: The Softmax Function\n",
    "\n",
    "### Step 1: Make Everything Positive\n",
    "\n",
    "Apply the exponential function to each logit:\n",
    "\n",
    "$$e^{z_j} > 0 \\quad \\text{for any } z_j \\in \\mathbb{R}$$\n",
    "\n",
    "This maps any real number to a positive number.\n",
    "\n",
    "### Step 2: Normalize\n",
    "\n",
    "Divide by the sum to ensure probabilities sum to 1:\n",
    "\n",
    "$$\\boxed{P(y=j|x) = \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}}$$\n",
    "\n",
    "This is the **softmax function**.\n",
    "\n",
    "### Why Exponential Specifically?\n",
    "\n",
    "The exponential function has a special property. If we compute the log-odds between two classes:\n",
    "\n",
    "$$\\log \\frac{P(y=j|x)}{P(y=k|x)} = \\log \\frac{e^{z_j}}{e^{z_k}} = z_j - z_k$$\n",
    "\n",
    "The **difference in logits equals the log-odds**. This gives a clean, interpretable relationship.\n",
    "\n",
    "### Properties of Softmax\n",
    "\n",
    "| Property | Explanation |\n",
    "|----------|-------------|\n",
    "| Outputs in $(0, 1)$ | Each output is a valid probability |\n",
    "| Sum to 1 | $\\sum_j P(y=j\\|x) = 1$ |\n",
    "| Preserves ranking | Higher logit → higher probability |\n",
    "| Differentiable | Enables gradient-based optimization |\n",
    "\n",
    "\n",
    "## Part 4: Maximum Likelihood Estimation\n",
    "\n",
    "### The Goal\n",
    "\n",
    "We have training data: $\\{(x_1, y_1), (x_2, y_2), \\ldots, (x_N, y_N)\\}$\n",
    "\n",
    "We want to find parameters $\\theta = \\{W_j, b_j\\}$ that best explain this data.\n",
    "\n",
    "### The Likelihood Function\n",
    "\n",
    "**Key question**: Given parameters $\\theta$, what is the probability that our model would produce the observed labels?\n",
    "\n",
    "For a single sample $(x_i, y_i)$:\n",
    "\n",
    "$$L_i(\\theta) = P(y_i | x_i; \\theta)$$\n",
    "\n",
    "This is the probability our model assigns to the **true class**.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose for sample $i$:\n",
    "- True class: \"cat\"\n",
    "- Model outputs: $P(\\text{cat}) = 0.8$, $P(\\text{dog}) = 0.15$, $P(\\text{bird}) = 0.05$\n",
    "\n",
    "Then $L_i(\\theta) = 0.8$\n",
    "\n",
    "### Likelihood for the Entire Dataset\n",
    "\n",
    "Assuming samples are **independent**, the joint probability is the product:\n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^{N} P(y_i | x_i; \\theta)$$\n",
    "\n",
    "**Why multiply?** For independent events A and B:\n",
    "\n",
    "$$P(A \\text{ and } B) = P(A) \\times P(B)$$\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Principle**: Choose parameters that maximize the likelihood.\n",
    "\n",
    "$$\\hat{\\theta} = \\arg\\max_{\\theta} \\prod_{i=1}^{N} P(y_i | x_i; \\theta)$$\n",
    "\n",
    "\n",
    "## Part 5: Log-Likelihood\n",
    "\n",
    "### The Numerical Problem\n",
    "\n",
    "With $N = 10000$ samples:\n",
    "\n",
    "$$L(\\theta) = 0.8 \\times 0.7 \\times 0.9 \\times \\ldots \\approx 10^{-500}$$\n",
    "\n",
    "This causes **numerical underflow** — computers can't represent such small numbers.\n",
    "\n",
    "### The Solution: Take the Logarithm\n",
    "\n",
    "Since $\\log$ is a **monotonically increasing function**:\n",
    "\n",
    "$$\\arg\\max_\\theta L(\\theta) = \\arg\\max_\\theta \\log L(\\theta)$$\n",
    "\n",
    "The parameters that maximize $L$ also maximize $\\log L$.\n",
    "\n",
    "### Log of a Product Becomes a Sum\n",
    "\n",
    "$$\\log L(\\theta) = \\log \\prod_{i=1}^{N} P(y_i|x_i) = \\sum_{i=1}^{N} \\log P(y_i|x_i)$$\n",
    "\n",
    "Now we're adding log-probabilities instead of multiplying probabilities. Much more stable!\n",
    "\n",
    "\n",
    "## Part 6: From Log-Likelihood to Cross-Entropy Loss\n",
    "\n",
    "### Maximizing vs. Minimizing\n",
    "\n",
    "In machine learning, we typically **minimize a loss function** rather than maximize likelihood.\n",
    "\n",
    "Simple fix — take the negative:\n",
    "\n",
    "$$\\text{Negative Log-Likelihood} = -\\log L(\\theta) = -\\sum_{i=1}^{N} \\log P(y_i|x_i)$$\n",
    "\n",
    "Minimizing this is equivalent to maximizing the likelihood.\n",
    "\n",
    "### Cross-Entropy Loss (Single Sample)\n",
    "\n",
    "For one sample with true class $y$:\n",
    "\n",
    "$$L_{CE} = -\\log P(y|x)$$\n",
    "\n",
    "Substituting the softmax:\n",
    "\n",
    "$$L_{CE} = -\\log \\frac{e^{z_y}}{\\sum_{k=1}^{C} e^{z_k}}$$\n",
    "\n",
    "### Expanding the Logarithm\n",
    "\n",
    "Using $\\log \\frac{a}{b} = \\log a - \\log b$:\n",
    "\n",
    "$$L_{CE} = -\\left( \\log e^{z_y} - \\log \\sum_{k=1}^{C} e^{z_k} \\right)$$\n",
    "\n",
    "$$\\boxed{L_{CE} = -z_y + \\log \\sum_{k=1}^{C} e^{z_k}}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- $-z_y$: We want the logit of the true class to be **high** (so this term is small)\n",
    "- $\\log \\sum_k e^{z_k}$: We want other logits to be **low** (so this term is small)\n",
    "\n",
    "\n",
    "## Part 7: General Form with One-Hot Encoding\n",
    "\n",
    "### One-Hot Representation\n",
    "\n",
    "Instead of representing the true class as a single integer $y$, we use a **one-hot vector**:\n",
    "\n",
    "$$\\mathbf{y} = [y_1, y_2, \\ldots, y_C]$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$y_j = \\begin{cases} 1 & \\text{if } j \\text{ is the true class} \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "For 3 classes (cat, dog, bird) where the true class is \"dog\":\n",
    "\n",
    "$$\\mathbf{y} = [0, 1, 0]$$\n",
    "\n",
    "### Cross-Entropy with One-Hot Encoding\n",
    "\n",
    "$$L_{CE} = -\\sum_{j=1}^{C} y_j \\log P(y=j|x)$$\n",
    "\n",
    "Since $y_j = 0$ for all classes except the true class, this simplifies to:\n",
    "\n",
    "$$L_{CE} = -\\log P(y=\\text{true class}|x)$$\n",
    "\n",
    "which matches our earlier formula.\n",
    "\n",
    "### Full Formula\n",
    "\n",
    "$$\\boxed{L_{CE} = -\\sum_{j=1}^{C} y_j \\log \\frac{e^{z_j}}{\\sum_{k=1}^{C} e^{z_k}}}$$\n",
    "\n",
    "\n",
    "## Part 8: Connection to Information Theory\n",
    "\n",
    "### What is Entropy?\n",
    "\n",
    "For a probability distribution $p$, **entropy** measures uncertainty:\n",
    "\n",
    "$$H(p) = -\\sum_j p_j \\log p_j$$\n",
    "\n",
    "### What is Cross-Entropy?\n",
    "\n",
    "**Cross-entropy** between true distribution $p$ and predicted distribution $q$:\n",
    "\n",
    "$$H(p, q) = -\\sum_j p_j \\log q_j$$\n",
    "\n",
    "In classification:\n",
    "- $p$ = one-hot vector (true distribution, all mass on one class)\n",
    "- $q$ = softmax output (predicted distribution)\n",
    "\n",
    "So our loss is literally the cross-entropy between true and predicted distributions!\n",
    "\n",
    "### Relationship to KL Divergence\n",
    "\n",
    "$$H(p, q) = H(p) + D_{KL}(p \\| q)$$\n",
    "\n",
    "Since $H(p) = 0$ for one-hot vectors:\n",
    "\n",
    "$$L_{CE} = D_{KL}(p \\| q)$$\n",
    "\n",
    "Minimizing cross-entropy = minimizing KL divergence from true to predicted distribution.\n",
    "\n",
    "\n",
    "## Gradient of Cross-Entropy Loss\n",
    "\n",
    "For backpropagation, we need $\\frac{\\partial L_{CE}}{\\partial z_j}$.\n",
    "\n",
    "### Result\n",
    "\n",
    "$$\\frac{\\partial L_{CE}}{\\partial z_j} = P(y=j|x) - y_j = \\text{softmax}(z_j) - y_j$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "| True class? | $y_j$ | $P(y=j\\|x)$ | Gradient |\n",
    "|-------------|-------|-------------|----------|\n",
    "| Yes | 1 | e.g., 0.8 | $0.8 - 1 = -0.2$ (push up) |\n",
    "| No | 0 | e.g., 0.15 | $0.15 - 0 = 0.15$ (push down) |\n",
    "\n",
    "The gradient:\n",
    "- **Increases** the logit of the true class\n",
    "- **Decreases** the logits of incorrect classes\n",
    "- Magnitude depends on how wrong the prediction is\n",
    "\n",
    "\n",
    "##  Summary\n",
    "\n",
    "| Step | What We Do | Why |\n",
    "|------|------------|-----|\n",
    "| 1. Compute logits | $z_j = W_j^T f + b_j$ | Linear transformation of features |\n",
    "| 2. Apply softmax | $P(j\\|x) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ | Convert to valid probabilities |\n",
    "| 3. Compute loss | $L = -\\log P(\\text{true class}\\|x)$ | Negative log-likelihood |\n",
    "| 4. Backpropagate | $\\nabla_z L = \\text{softmax}(z) - y$ | Update parameters |\n",
    "\n",
    "### The Complete Pipeline\n",
    "\n",
    "```\n",
    "Input x\n",
    "    ↓\n",
    "Neural Network → Feature f\n",
    "    ↓\n",
    "Linear Layer → Logits z = Wf + b\n",
    "    ↓\n",
    "Softmax → Probabilities P(y|x)\n",
    "    ↓\n",
    "Cross-Entropy Loss with true label y\n",
    "    ↓\n",
    "Backpropagation to update W, b\n",
    "```\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Softmax** converts unconstrained logits to valid probabilities\n",
    "2. **Cross-entropy loss** comes from maximum likelihood estimation\n",
    "3. **Log transformation** converts products to sums for numerical stability\n",
    "4. **Negative sign** converts maximization to minimization\n",
    "5. The gradient has an elegant form: $\\text{prediction} - \\text{target}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532c989",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
